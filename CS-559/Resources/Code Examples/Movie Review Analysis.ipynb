{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
      "1  OK... so... I really like Kris Kristofferson a...          0\n",
      "2  ***SPOILER*** Do not read this, if you think a...          0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:51\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing the data:\n",
    "## Separate words and \n",
    "## count each word's occurrence\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Counting words occurences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' \\\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:03\n"
     ]
    }
   ],
   "source": [
    "## Create a mapping:\n",
    "## Map each unique word to an integer\n",
    "\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in \\\n",
    "               enumerate(word_counts, 1)}\n",
    "\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define fixed-length sequences:\n",
    "## Use the last 200 elements of each sequence\n",
    "## if sequence length < 200: left-pad with zeros\n",
    "\n",
    "sequence_length = 200  ## sequence length (or T in our formulas)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "\n",
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "\n",
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "## Function to generate minibatches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x= x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size=256, num_layers=1, batch_size=64,\n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size   ## number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self):\n",
    "        ## Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                    shape=(self.batch_size, self.seq_len),\n",
    "                    name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                    shape=(self.batch_size),\n",
    "                    name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                    name='tf_keepprob')\n",
    "        ## Create the embedding layer\n",
    "        embedding = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        (self.n_words, self.embed_size),\n",
    "                        minval=-1, maxval=1),\n",
    "                    name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(\n",
    "                    embedding, tf_x, \n",
    "                    name='embeded_x')\n",
    "\n",
    "        ## Define LSTM cell and stack them together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                [tf.contrib.rnn.DropoutWrapper(\n",
    "                   tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                   output_keep_prob=tf_keepprob)\n",
    "                 for i in range(self.num_layers)])\n",
    "\n",
    "        ## Define the initial state:\n",
    "        self.initial_state = cells.zero_state(\n",
    "                 self.batch_size, tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                 cells, embed_x,\n",
    "                 initial_state=self.initial_state)\n",
    "        ## Note: lstm_outputs shape: \n",
    "        ##  [batch_size, max_time, cells.output_size]\n",
    "        print('\\n  << lstm_output   >> ', lstm_outputs)\n",
    "        print('\\n  << final state   >> ', self.final_state)\n",
    "\n",
    "        ## Apply a FC layer after on top of RNN output:\n",
    "        logits = tf.layers.dense(\n",
    "                 inputs=lstm_outputs[:, -1],\n",
    "                 units=1, activation=None,\n",
    "                 name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print ('\\n  << logits        >> ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
    "                 name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >> ', predictions)\n",
    "\n",
    "        ## Define the cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                 tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                 labels=tf_y, logits=logits),\n",
    "                 name='cost')\n",
    "        \n",
    "        ## Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                            X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state : state}\n",
    "                    loss, _, state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                             self.final_state],\n",
    "                            feed_dict=feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                               epoch + 1, num_epochs,\n",
    "                               iteration, loss))\n",
    "\n",
    "                    iteration +=1\n",
    "                if (epoch+1)%10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                        \"model/sentiment-%d.ckpt\" % epoch)\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(\n",
    "                create_batch_generator(\n",
    "                    X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0' : batch_x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state : test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                preds.append(pred)\n",
    "                \n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-9-4718ce0769f6>:45: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-4718ce0769f6>:45: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "  << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "WARNING:tensorflow:From <ipython-input-9-4718ce0769f6>:54: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "  << lstm_output   >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state   >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "WARNING:tensorflow:From <ipython-input-9-4718ce0769f6>:64: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "\n",
      "  << logits        >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words, \n",
    "                   seq_len=sequence_length,\n",
    "                   embed_size=256, \n",
    "                   lstm_size=128, \n",
    "                   num_layers=1, \n",
    "                   batch_size=100, \n",
    "                   learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 Iteration: 20 | Train loss: 0.70256\n",
      "Epoch: 1/40 Iteration: 40 | Train loss: 0.69832\n",
      "Epoch: 1/40 Iteration: 60 | Train loss: 0.70363\n",
      "Epoch: 1/40 Iteration: 80 | Train loss: 0.68928\n",
      "Epoch: 1/40 Iteration: 100 | Train loss: 0.70398\n",
      "Epoch: 1/40 Iteration: 120 | Train loss: 0.71784\n",
      "Epoch: 1/40 Iteration: 140 | Train loss: 0.67034\n",
      "Epoch: 1/40 Iteration: 160 | Train loss: 0.67440\n",
      "Epoch: 1/40 Iteration: 180 | Train loss: 0.68110\n",
      "Epoch: 1/40 Iteration: 200 | Train loss: 0.67271\n",
      "Epoch: 1/40 Iteration: 220 | Train loss: 0.63612\n",
      "Epoch: 1/40 Iteration: 240 | Train loss: 0.66422\n",
      "Epoch: 2/40 Iteration: 260 | Train loss: 0.64080\n",
      "Epoch: 2/40 Iteration: 280 | Train loss: 0.59376\n",
      "Epoch: 2/40 Iteration: 300 | Train loss: 0.49504\n",
      "Epoch: 2/40 Iteration: 320 | Train loss: 0.54293\n",
      "Epoch: 2/40 Iteration: 340 | Train loss: 0.60307\n",
      "Epoch: 2/40 Iteration: 360 | Train loss: 0.40400\n",
      "Epoch: 2/40 Iteration: 380 | Train loss: 0.56816\n",
      "Epoch: 2/40 Iteration: 400 | Train loss: 0.48348\n",
      "Epoch: 2/40 Iteration: 420 | Train loss: 0.51509\n",
      "Epoch: 2/40 Iteration: 440 | Train loss: 0.51920\n",
      "Epoch: 2/40 Iteration: 460 | Train loss: 0.62810\n",
      "Epoch: 2/40 Iteration: 480 | Train loss: 0.52762\n",
      "Epoch: 2/40 Iteration: 500 | Train loss: 0.39504\n",
      "Epoch: 3/40 Iteration: 520 | Train loss: 0.48078\n",
      "Epoch: 3/40 Iteration: 540 | Train loss: 0.50356\n",
      "Epoch: 3/40 Iteration: 560 | Train loss: 0.53393\n",
      "Epoch: 3/40 Iteration: 580 | Train loss: 0.49058\n",
      "Epoch: 3/40 Iteration: 600 | Train loss: 0.50991\n",
      "Epoch: 3/40 Iteration: 620 | Train loss: 0.46011\n",
      "Epoch: 3/40 Iteration: 640 | Train loss: 0.37914\n",
      "Epoch: 3/40 Iteration: 660 | Train loss: 0.39085\n",
      "Epoch: 3/40 Iteration: 680 | Train loss: 0.42762\n",
      "Epoch: 3/40 Iteration: 700 | Train loss: 0.39840\n",
      "Epoch: 3/40 Iteration: 720 | Train loss: 0.40618\n",
      "Epoch: 3/40 Iteration: 740 | Train loss: 0.46621\n",
      "Epoch: 4/40 Iteration: 760 | Train loss: 0.39129\n",
      "Epoch: 4/40 Iteration: 780 | Train loss: 0.36211\n",
      "Epoch: 4/40 Iteration: 800 | Train loss: 0.35182\n",
      "Epoch: 4/40 Iteration: 820 | Train loss: 0.40977\n",
      "Epoch: 4/40 Iteration: 840 | Train loss: 0.42248\n",
      "Epoch: 4/40 Iteration: 860 | Train loss: 0.28454\n",
      "Epoch: 4/40 Iteration: 880 | Train loss: 0.41867\n",
      "Epoch: 4/40 Iteration: 900 | Train loss: 0.37935\n",
      "Epoch: 4/40 Iteration: 920 | Train loss: 0.37298\n",
      "Epoch: 4/40 Iteration: 940 | Train loss: 0.37349\n",
      "Epoch: 4/40 Iteration: 960 | Train loss: 0.51611\n",
      "Epoch: 4/40 Iteration: 980 | Train loss: 0.37575\n",
      "Epoch: 4/40 Iteration: 1000 | Train loss: 0.27684\n",
      "Epoch: 5/40 Iteration: 1020 | Train loss: 0.38511\n",
      "Epoch: 5/40 Iteration: 1040 | Train loss: 0.39069\n",
      "Epoch: 5/40 Iteration: 1060 | Train loss: 0.45196\n",
      "Epoch: 5/40 Iteration: 1080 | Train loss: 0.36502\n",
      "Epoch: 5/40 Iteration: 1100 | Train loss: 0.39664\n",
      "Epoch: 5/40 Iteration: 1120 | Train loss: 0.36515\n",
      "Epoch: 5/40 Iteration: 1140 | Train loss: 0.30673\n",
      "Epoch: 5/40 Iteration: 1160 | Train loss: 0.30358\n",
      "Epoch: 5/40 Iteration: 1180 | Train loss: 0.31551\n",
      "Epoch: 5/40 Iteration: 1200 | Train loss: 0.33919\n",
      "Epoch: 5/40 Iteration: 1220 | Train loss: 0.28556\n",
      "Epoch: 5/40 Iteration: 1240 | Train loss: 0.36178\n",
      "Epoch: 6/40 Iteration: 1260 | Train loss: 0.33842\n",
      "Epoch: 6/40 Iteration: 1280 | Train loss: 0.27410\n",
      "Epoch: 6/40 Iteration: 1300 | Train loss: 0.31560\n",
      "Epoch: 6/40 Iteration: 1320 | Train loss: 0.39564\n",
      "Epoch: 6/40 Iteration: 1340 | Train loss: 0.32272\n",
      "Epoch: 6/40 Iteration: 1360 | Train loss: 0.23863\n",
      "Epoch: 6/40 Iteration: 1380 | Train loss: 0.35033\n",
      "Epoch: 6/40 Iteration: 1400 | Train loss: 0.34995\n",
      "Epoch: 6/40 Iteration: 1420 | Train loss: 0.32135\n",
      "Epoch: 6/40 Iteration: 1440 | Train loss: 0.30822\n",
      "Epoch: 6/40 Iteration: 1460 | Train loss: 0.46822\n",
      "Epoch: 6/40 Iteration: 1480 | Train loss: 0.30900\n",
      "Epoch: 6/40 Iteration: 1500 | Train loss: 0.23396\n",
      "Epoch: 7/40 Iteration: 1520 | Train loss: 0.32939\n",
      "Epoch: 7/40 Iteration: 1540 | Train loss: 0.33412\n",
      "Epoch: 7/40 Iteration: 1560 | Train loss: 0.35052\n",
      "Epoch: 7/40 Iteration: 1580 | Train loss: 0.30072\n",
      "Epoch: 7/40 Iteration: 1600 | Train loss: 0.34919\n",
      "Epoch: 7/40 Iteration: 1620 | Train loss: 0.29441\n",
      "Epoch: 7/40 Iteration: 1640 | Train loss: 0.23553\n",
      "Epoch: 7/40 Iteration: 1660 | Train loss: 0.25138\n",
      "Epoch: 7/40 Iteration: 1680 | Train loss: 0.25548\n",
      "Epoch: 7/40 Iteration: 1700 | Train loss: 0.26469\n",
      "Epoch: 7/40 Iteration: 1720 | Train loss: 0.25097\n",
      "Epoch: 7/40 Iteration: 1740 | Train loss: 0.33862\n",
      "Epoch: 8/40 Iteration: 1760 | Train loss: 0.28027\n",
      "Epoch: 8/40 Iteration: 1780 | Train loss: 0.21847\n",
      "Epoch: 8/40 Iteration: 1800 | Train loss: 0.26356\n",
      "Epoch: 8/40 Iteration: 1820 | Train loss: 0.33073\n",
      "Epoch: 8/40 Iteration: 1840 | Train loss: 0.23061\n",
      "Epoch: 8/40 Iteration: 1860 | Train loss: 0.18911\n",
      "Epoch: 8/40 Iteration: 1880 | Train loss: 0.28137\n",
      "Epoch: 8/40 Iteration: 1900 | Train loss: 0.32137\n",
      "Epoch: 8/40 Iteration: 1920 | Train loss: 0.25705\n",
      "Epoch: 8/40 Iteration: 1940 | Train loss: 0.27207\n",
      "Epoch: 8/40 Iteration: 1960 | Train loss: 0.40161\n",
      "Epoch: 8/40 Iteration: 1980 | Train loss: 0.25188\n",
      "Epoch: 8/40 Iteration: 2000 | Train loss: 0.22416\n",
      "Epoch: 9/40 Iteration: 2020 | Train loss: 0.30454\n",
      "Epoch: 9/40 Iteration: 2040 | Train loss: 0.28794\n",
      "Epoch: 9/40 Iteration: 2060 | Train loss: 0.25972\n",
      "Epoch: 9/40 Iteration: 2080 | Train loss: 0.25278\n",
      "Epoch: 9/40 Iteration: 2100 | Train loss: 0.28913\n",
      "Epoch: 9/40 Iteration: 2120 | Train loss: 0.22749\n",
      "Epoch: 9/40 Iteration: 2140 | Train loss: 0.18964\n",
      "Epoch: 9/40 Iteration: 2160 | Train loss: 0.21430\n",
      "Epoch: 9/40 Iteration: 2180 | Train loss: 0.17644\n",
      "Epoch: 9/40 Iteration: 2200 | Train loss: 0.21327\n",
      "Epoch: 9/40 Iteration: 2220 | Train loss: 0.18685\n",
      "Epoch: 9/40 Iteration: 2240 | Train loss: 0.27802\n",
      "Epoch: 10/40 Iteration: 2260 | Train loss: 0.25516\n",
      "Epoch: 10/40 Iteration: 2280 | Train loss: 0.15334\n",
      "Epoch: 10/40 Iteration: 2300 | Train loss: 0.19388\n",
      "Epoch: 10/40 Iteration: 2320 | Train loss: 0.24040\n",
      "Epoch: 10/40 Iteration: 2340 | Train loss: 0.16351\n",
      "Epoch: 10/40 Iteration: 2360 | Train loss: 0.14491\n",
      "Epoch: 10/40 Iteration: 2380 | Train loss: 0.26053\n",
      "Epoch: 10/40 Iteration: 2400 | Train loss: 0.26674\n",
      "Epoch: 10/40 Iteration: 2420 | Train loss: 0.22240\n",
      "Epoch: 10/40 Iteration: 2440 | Train loss: 0.19721\n",
      "Epoch: 10/40 Iteration: 2460 | Train loss: 0.33300\n",
      "Epoch: 10/40 Iteration: 2480 | Train loss: 0.19564\n",
      "Epoch: 10/40 Iteration: 2500 | Train loss: 0.16675\n",
      "Epoch: 11/40 Iteration: 2520 | Train loss: 0.24794\n",
      "Epoch: 11/40 Iteration: 2540 | Train loss: 0.22108\n",
      "Epoch: 11/40 Iteration: 2560 | Train loss: 0.23483\n",
      "Epoch: 11/40 Iteration: 2580 | Train loss: 0.20168\n",
      "Epoch: 11/40 Iteration: 2600 | Train loss: 0.23459\n",
      "Epoch: 11/40 Iteration: 2620 | Train loss: 0.20018\n",
      "Epoch: 11/40 Iteration: 2640 | Train loss: 0.15040\n",
      "Epoch: 11/40 Iteration: 2660 | Train loss: 0.16307\n",
      "Epoch: 11/40 Iteration: 2680 | Train loss: 0.12659\n",
      "Epoch: 11/40 Iteration: 2700 | Train loss: 0.17380\n",
      "Epoch: 11/40 Iteration: 2720 | Train loss: 0.11302\n",
      "Epoch: 11/40 Iteration: 2740 | Train loss: 0.28443\n",
      "Epoch: 12/40 Iteration: 2760 | Train loss: 0.19676\n",
      "Epoch: 12/40 Iteration: 2780 | Train loss: 0.13257\n",
      "Epoch: 12/40 Iteration: 2800 | Train loss: 0.18974\n",
      "Epoch: 12/40 Iteration: 2820 | Train loss: 0.19701\n",
      "Epoch: 12/40 Iteration: 2840 | Train loss: 0.14067\n",
      "Epoch: 12/40 Iteration: 2860 | Train loss: 0.13763\n",
      "Epoch: 12/40 Iteration: 2880 | Train loss: 0.23966\n",
      "Epoch: 12/40 Iteration: 2900 | Train loss: 0.24068\n",
      "Epoch: 12/40 Iteration: 2920 | Train loss: 0.20281\n",
      "Epoch: 12/40 Iteration: 2940 | Train loss: 0.23735\n",
      "Epoch: 12/40 Iteration: 2960 | Train loss: 0.22711\n",
      "Epoch: 12/40 Iteration: 2980 | Train loss: 0.17964\n",
      "Epoch: 12/40 Iteration: 3000 | Train loss: 0.10152\n",
      "Epoch: 13/40 Iteration: 3020 | Train loss: 0.23460\n",
      "Epoch: 13/40 Iteration: 3040 | Train loss: 0.16177\n",
      "Epoch: 13/40 Iteration: 3060 | Train loss: 0.21901\n",
      "Epoch: 13/40 Iteration: 3080 | Train loss: 0.17138\n",
      "Epoch: 13/40 Iteration: 3100 | Train loss: 0.21472\n",
      "Epoch: 13/40 Iteration: 3120 | Train loss: 0.15996\n",
      "Epoch: 13/40 Iteration: 3140 | Train loss: 0.21545\n",
      "Epoch: 13/40 Iteration: 3160 | Train loss: 0.13849\n",
      "Epoch: 13/40 Iteration: 3180 | Train loss: 0.16733\n",
      "Epoch: 13/40 Iteration: 3200 | Train loss: 0.20013\n",
      "Epoch: 13/40 Iteration: 3220 | Train loss: 0.10187\n",
      "Epoch: 13/40 Iteration: 3240 | Train loss: 0.22336\n",
      "Epoch: 14/40 Iteration: 3260 | Train loss: 0.14895\n",
      "Epoch: 14/40 Iteration: 3280 | Train loss: 0.13957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 Iteration: 3300 | Train loss: 0.13662\n",
      "Epoch: 14/40 Iteration: 3320 | Train loss: 0.16117\n",
      "Epoch: 14/40 Iteration: 3340 | Train loss: 0.06032\n",
      "Epoch: 14/40 Iteration: 3360 | Train loss: 0.16183\n",
      "Epoch: 14/40 Iteration: 3380 | Train loss: 0.21481\n",
      "Epoch: 14/40 Iteration: 3400 | Train loss: 0.27538\n",
      "Epoch: 14/40 Iteration: 3420 | Train loss: 0.20015\n",
      "Epoch: 14/40 Iteration: 3440 | Train loss: 0.22401\n",
      "Epoch: 14/40 Iteration: 3460 | Train loss: 0.23279\n",
      "Epoch: 14/40 Iteration: 3480 | Train loss: 0.14454\n",
      "Epoch: 14/40 Iteration: 3500 | Train loss: 0.06925\n",
      "Epoch: 15/40 Iteration: 3520 | Train loss: 0.21045\n",
      "Epoch: 15/40 Iteration: 3540 | Train loss: 0.14272\n",
      "Epoch: 15/40 Iteration: 3560 | Train loss: 0.16046\n",
      "Epoch: 15/40 Iteration: 3580 | Train loss: 0.17974\n",
      "Epoch: 15/40 Iteration: 3600 | Train loss: 0.19338\n",
      "Epoch: 15/40 Iteration: 3620 | Train loss: 0.16239\n",
      "Epoch: 15/40 Iteration: 3640 | Train loss: 0.11042\n",
      "Epoch: 15/40 Iteration: 3660 | Train loss: 0.13929\n",
      "Epoch: 15/40 Iteration: 3680 | Train loss: 0.11434\n",
      "Epoch: 15/40 Iteration: 3700 | Train loss: 0.11051\n",
      "Epoch: 15/40 Iteration: 3720 | Train loss: 0.07491\n",
      "Epoch: 15/40 Iteration: 3740 | Train loss: 0.18819\n",
      "Epoch: 16/40 Iteration: 3760 | Train loss: 0.17801\n",
      "Epoch: 16/40 Iteration: 3780 | Train loss: 0.05463\n",
      "Epoch: 16/40 Iteration: 3800 | Train loss: 0.11459\n",
      "Epoch: 16/40 Iteration: 3820 | Train loss: 0.12161\n",
      "Epoch: 16/40 Iteration: 3840 | Train loss: 0.12874\n",
      "Epoch: 16/40 Iteration: 3860 | Train loss: 0.17003\n",
      "Epoch: 16/40 Iteration: 3880 | Train loss: 0.16677\n",
      "Epoch: 16/40 Iteration: 3900 | Train loss: 0.19800\n",
      "Epoch: 16/40 Iteration: 3920 | Train loss: 0.18694\n",
      "Epoch: 16/40 Iteration: 3940 | Train loss: 0.22843\n",
      "Epoch: 16/40 Iteration: 3960 | Train loss: 0.16982\n",
      "Epoch: 16/40 Iteration: 3980 | Train loss: 0.16094\n",
      "Epoch: 16/40 Iteration: 4000 | Train loss: 0.06420\n",
      "Epoch: 17/40 Iteration: 4020 | Train loss: 0.19682\n",
      "Epoch: 17/40 Iteration: 4040 | Train loss: 0.11898\n",
      "Epoch: 17/40 Iteration: 4060 | Train loss: 0.11091\n",
      "Epoch: 17/40 Iteration: 4080 | Train loss: 0.16032\n",
      "Epoch: 17/40 Iteration: 4100 | Train loss: 0.15440\n",
      "Epoch: 17/40 Iteration: 4120 | Train loss: 0.13654\n",
      "Epoch: 17/40 Iteration: 4140 | Train loss: 0.05881\n",
      "Epoch: 17/40 Iteration: 4160 | Train loss: 0.07357\n",
      "Epoch: 17/40 Iteration: 4180 | Train loss: 0.06097\n",
      "Epoch: 17/40 Iteration: 4200 | Train loss: 0.09321\n",
      "Epoch: 17/40 Iteration: 4220 | Train loss: 0.03726\n",
      "Epoch: 17/40 Iteration: 4240 | Train loss: 0.13654\n",
      "Epoch: 18/40 Iteration: 4260 | Train loss: 0.08601\n",
      "Epoch: 18/40 Iteration: 4280 | Train loss: 0.05903\n",
      "Epoch: 18/40 Iteration: 4300 | Train loss: 0.07558\n",
      "Epoch: 18/40 Iteration: 4320 | Train loss: 0.09797\n",
      "Epoch: 18/40 Iteration: 4340 | Train loss: 0.06663\n",
      "Epoch: 18/40 Iteration: 4360 | Train loss: 0.11348\n",
      "Epoch: 18/40 Iteration: 4380 | Train loss: 0.15747\n",
      "Epoch: 18/40 Iteration: 4400 | Train loss: 0.17034\n",
      "Epoch: 18/40 Iteration: 4420 | Train loss: 0.13364\n",
      "Epoch: 18/40 Iteration: 4440 | Train loss: 0.19172\n",
      "Epoch: 18/40 Iteration: 4460 | Train loss: 0.14315\n",
      "Epoch: 18/40 Iteration: 4480 | Train loss: 0.12031\n",
      "Epoch: 18/40 Iteration: 4500 | Train loss: 0.03549\n",
      "Epoch: 19/40 Iteration: 4520 | Train loss: 0.16524\n",
      "Epoch: 19/40 Iteration: 4540 | Train loss: 0.09009\n",
      "Epoch: 19/40 Iteration: 4560 | Train loss: 0.11377\n",
      "Epoch: 19/40 Iteration: 4580 | Train loss: 0.11959\n",
      "Epoch: 19/40 Iteration: 4600 | Train loss: 0.12513\n",
      "Epoch: 19/40 Iteration: 4620 | Train loss: 0.09902\n",
      "Epoch: 19/40 Iteration: 4640 | Train loss: 0.04741\n",
      "Epoch: 19/40 Iteration: 4660 | Train loss: 0.05899\n",
      "Epoch: 19/40 Iteration: 4680 | Train loss: 0.04208\n",
      "Epoch: 19/40 Iteration: 4700 | Train loss: 0.04937\n",
      "Epoch: 19/40 Iteration: 4720 | Train loss: 0.01213\n",
      "Epoch: 19/40 Iteration: 4740 | Train loss: 0.10455\n",
      "Epoch: 20/40 Iteration: 4760 | Train loss: 0.06598\n",
      "Epoch: 20/40 Iteration: 4780 | Train loss: 0.03510\n",
      "Epoch: 20/40 Iteration: 4800 | Train loss: 0.05652\n",
      "Epoch: 20/40 Iteration: 4820 | Train loss: 0.09181\n",
      "Epoch: 20/40 Iteration: 4840 | Train loss: 0.04980\n",
      "Epoch: 20/40 Iteration: 4860 | Train loss: 0.09918\n",
      "Epoch: 20/40 Iteration: 4880 | Train loss: 0.15315\n",
      "Epoch: 20/40 Iteration: 4900 | Train loss: 0.12971\n",
      "Epoch: 20/40 Iteration: 4920 | Train loss: 0.13028\n",
      "Epoch: 20/40 Iteration: 4940 | Train loss: 0.18171\n",
      "Epoch: 20/40 Iteration: 4960 | Train loss: 0.10969\n",
      "Epoch: 20/40 Iteration: 4980 | Train loss: 0.14168\n",
      "Epoch: 20/40 Iteration: 5000 | Train loss: 0.02551\n",
      "Epoch: 21/40 Iteration: 5020 | Train loss: 0.16439\n",
      "Epoch: 21/40 Iteration: 5040 | Train loss: 0.05077\n",
      "Epoch: 21/40 Iteration: 5060 | Train loss: 0.04192\n",
      "Epoch: 21/40 Iteration: 5080 | Train loss: 0.22767\n",
      "Epoch: 21/40 Iteration: 5100 | Train loss: 0.10637\n",
      "Epoch: 21/40 Iteration: 5120 | Train loss: 0.21115\n",
      "Epoch: 21/40 Iteration: 5140 | Train loss: 0.06707\n",
      "Epoch: 21/40 Iteration: 5160 | Train loss: 0.05480\n",
      "Epoch: 21/40 Iteration: 5180 | Train loss: 0.04534\n",
      "Epoch: 21/40 Iteration: 5200 | Train loss: 0.05022\n",
      "Epoch: 21/40 Iteration: 5220 | Train loss: 0.03656\n",
      "Epoch: 21/40 Iteration: 5240 | Train loss: 0.14541\n",
      "Epoch: 22/40 Iteration: 5260 | Train loss: 0.03150\n",
      "Epoch: 22/40 Iteration: 5280 | Train loss: 0.01530\n",
      "Epoch: 22/40 Iteration: 5300 | Train loss: 0.07154\n",
      "Epoch: 22/40 Iteration: 5320 | Train loss: 0.21619\n",
      "Epoch: 22/40 Iteration: 5340 | Train loss: 0.21704\n",
      "Epoch: 22/40 Iteration: 5360 | Train loss: 0.14757\n",
      "Epoch: 22/40 Iteration: 5380 | Train loss: 0.17759\n",
      "Epoch: 22/40 Iteration: 5400 | Train loss: 0.14181\n",
      "Epoch: 22/40 Iteration: 5420 | Train loss: 0.12886\n",
      "Epoch: 22/40 Iteration: 5440 | Train loss: 0.11607\n",
      "Epoch: 22/40 Iteration: 5460 | Train loss: 0.08879\n",
      "Epoch: 22/40 Iteration: 5480 | Train loss: 0.14513\n",
      "Epoch: 22/40 Iteration: 5500 | Train loss: 0.03371\n",
      "Epoch: 23/40 Iteration: 5520 | Train loss: 0.13057\n",
      "Epoch: 23/40 Iteration: 5540 | Train loss: 0.03945\n",
      "Epoch: 23/40 Iteration: 5560 | Train loss: 0.01709\n",
      "Epoch: 23/40 Iteration: 5580 | Train loss: 0.09331\n",
      "Epoch: 23/40 Iteration: 5600 | Train loss: 0.06539\n",
      "Epoch: 23/40 Iteration: 5620 | Train loss: 0.14084\n",
      "Epoch: 23/40 Iteration: 5640 | Train loss: 0.12770\n",
      "Epoch: 23/40 Iteration: 5660 | Train loss: 0.11421\n",
      "Epoch: 23/40 Iteration: 5680 | Train loss: 0.09017\n",
      "Epoch: 23/40 Iteration: 5700 | Train loss: 0.03534\n",
      "Epoch: 23/40 Iteration: 5720 | Train loss: 0.01306\n",
      "Epoch: 23/40 Iteration: 5740 | Train loss: 0.13819\n",
      "Epoch: 24/40 Iteration: 5760 | Train loss: 0.06371\n",
      "Epoch: 24/40 Iteration: 5780 | Train loss: 0.02104\n",
      "Epoch: 24/40 Iteration: 5800 | Train loss: 0.03934\n",
      "Epoch: 24/40 Iteration: 5820 | Train loss: 0.07931\n",
      "Epoch: 24/40 Iteration: 5840 | Train loss: 0.01321\n",
      "Epoch: 24/40 Iteration: 5860 | Train loss: 0.03995\n",
      "Epoch: 24/40 Iteration: 5880 | Train loss: 0.11804\n",
      "Epoch: 24/40 Iteration: 5900 | Train loss: 0.11750\n",
      "Epoch: 24/40 Iteration: 5920 | Train loss: 0.20509\n",
      "Epoch: 24/40 Iteration: 5940 | Train loss: 0.18554\n",
      "Epoch: 24/40 Iteration: 5960 | Train loss: 0.07445\n",
      "Epoch: 24/40 Iteration: 5980 | Train loss: 0.09361\n",
      "Epoch: 24/40 Iteration: 6000 | Train loss: 0.04637\n",
      "Epoch: 25/40 Iteration: 6020 | Train loss: 0.10194\n",
      "Epoch: 25/40 Iteration: 6040 | Train loss: 0.02618\n",
      "Epoch: 25/40 Iteration: 6060 | Train loss: 0.01794\n",
      "Epoch: 25/40 Iteration: 6080 | Train loss: 0.10136\n",
      "Epoch: 25/40 Iteration: 6100 | Train loss: 0.06526\n",
      "Epoch: 25/40 Iteration: 6120 | Train loss: 0.07698\n",
      "Epoch: 25/40 Iteration: 6140 | Train loss: 0.02063\n",
      "Epoch: 25/40 Iteration: 6160 | Train loss: 0.04630\n",
      "Epoch: 25/40 Iteration: 6180 | Train loss: 0.10483\n",
      "Epoch: 25/40 Iteration: 6200 | Train loss: 0.04626\n",
      "Epoch: 25/40 Iteration: 6220 | Train loss: 0.02444\n",
      "Epoch: 25/40 Iteration: 6240 | Train loss: 0.17442\n",
      "Epoch: 26/40 Iteration: 6260 | Train loss: 0.12487\n",
      "Epoch: 26/40 Iteration: 6280 | Train loss: 0.02380\n",
      "Epoch: 26/40 Iteration: 6300 | Train loss: 0.04240\n",
      "Epoch: 26/40 Iteration: 6320 | Train loss: 0.03570\n",
      "Epoch: 26/40 Iteration: 6340 | Train loss: 0.01087\n",
      "Epoch: 26/40 Iteration: 6360 | Train loss: 0.04068\n",
      "Epoch: 26/40 Iteration: 6380 | Train loss: 0.11229\n",
      "Epoch: 26/40 Iteration: 6400 | Train loss: 0.13315\n",
      "Epoch: 26/40 Iteration: 6420 | Train loss: 0.18628\n",
      "Epoch: 26/40 Iteration: 6440 | Train loss: 0.13220\n",
      "Epoch: 26/40 Iteration: 6460 | Train loss: 0.04882\n",
      "Epoch: 26/40 Iteration: 6480 | Train loss: 0.10664\n",
      "Epoch: 26/40 Iteration: 6500 | Train loss: 0.04534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/40 Iteration: 6520 | Train loss: 0.08153\n",
      "Epoch: 27/40 Iteration: 6540 | Train loss: 0.02092\n",
      "Epoch: 27/40 Iteration: 6560 | Train loss: 0.00898\n",
      "Epoch: 27/40 Iteration: 6580 | Train loss: 0.09037\n",
      "Epoch: 27/40 Iteration: 6600 | Train loss: 0.05836\n",
      "Epoch: 27/40 Iteration: 6620 | Train loss: 0.06412\n",
      "Epoch: 27/40 Iteration: 6640 | Train loss: 0.03503\n",
      "Epoch: 27/40 Iteration: 6660 | Train loss: 0.03522\n",
      "Epoch: 27/40 Iteration: 6680 | Train loss: 0.09101\n",
      "Epoch: 27/40 Iteration: 6700 | Train loss: 0.02003\n",
      "Epoch: 27/40 Iteration: 6720 | Train loss: 0.00767\n",
      "Epoch: 27/40 Iteration: 6740 | Train loss: 0.10520\n",
      "Epoch: 28/40 Iteration: 6760 | Train loss: 0.06725\n",
      "Epoch: 28/40 Iteration: 6780 | Train loss: 0.04734\n",
      "Epoch: 28/40 Iteration: 6800 | Train loss: 0.03642\n",
      "Epoch: 28/40 Iteration: 6820 | Train loss: 0.01714\n",
      "Epoch: 28/40 Iteration: 6840 | Train loss: 0.01212\n",
      "Epoch: 28/40 Iteration: 6860 | Train loss: 0.02874\n",
      "Epoch: 28/40 Iteration: 6880 | Train loss: 0.08180\n",
      "Epoch: 28/40 Iteration: 6900 | Train loss: 0.06628\n",
      "Epoch: 28/40 Iteration: 6920 | Train loss: 0.07734\n",
      "Epoch: 28/40 Iteration: 6940 | Train loss: 0.05875\n",
      "Epoch: 28/40 Iteration: 6960 | Train loss: 0.16075\n",
      "Epoch: 28/40 Iteration: 6980 | Train loss: 0.09096\n",
      "Epoch: 28/40 Iteration: 7000 | Train loss: 0.02572\n",
      "Epoch: 29/40 Iteration: 7020 | Train loss: 0.10408\n",
      "Epoch: 29/40 Iteration: 7040 | Train loss: 0.03272\n",
      "Epoch: 29/40 Iteration: 7060 | Train loss: 0.00917\n",
      "Epoch: 29/40 Iteration: 7080 | Train loss: 0.10797\n",
      "Epoch: 29/40 Iteration: 7100 | Train loss: 0.04794\n",
      "Epoch: 29/40 Iteration: 7120 | Train loss: 0.03622\n",
      "Epoch: 29/40 Iteration: 7140 | Train loss: 0.01120\n",
      "Epoch: 29/40 Iteration: 7160 | Train loss: 0.01876\n",
      "Epoch: 29/40 Iteration: 7180 | Train loss: 0.01641\n",
      "Epoch: 29/40 Iteration: 7200 | Train loss: 0.02811\n",
      "Epoch: 29/40 Iteration: 7220 | Train loss: 0.20054\n",
      "Epoch: 29/40 Iteration: 7240 | Train loss: 0.18607\n",
      "Epoch: 30/40 Iteration: 7260 | Train loss: 0.14027\n",
      "Epoch: 30/40 Iteration: 7280 | Train loss: 0.01308\n",
      "Epoch: 30/40 Iteration: 7300 | Train loss: 0.01603\n",
      "Epoch: 30/40 Iteration: 7320 | Train loss: 0.03073\n",
      "Epoch: 30/40 Iteration: 7340 | Train loss: 0.00880\n",
      "Epoch: 30/40 Iteration: 7360 | Train loss: 0.02980\n",
      "Epoch: 30/40 Iteration: 7380 | Train loss: 0.07737\n",
      "Epoch: 30/40 Iteration: 7400 | Train loss: 0.06773\n",
      "Epoch: 30/40 Iteration: 7420 | Train loss: 0.12337\n",
      "Epoch: 30/40 Iteration: 7440 | Train loss: 0.02530\n",
      "Epoch: 30/40 Iteration: 7460 | Train loss: 0.03672\n",
      "Epoch: 30/40 Iteration: 7480 | Train loss: 0.03402\n",
      "Epoch: 30/40 Iteration: 7500 | Train loss: 0.10425\n",
      "Epoch: 31/40 Iteration: 7520 | Train loss: 0.03543\n",
      "Epoch: 31/40 Iteration: 7540 | Train loss: 0.01327\n",
      "Epoch: 31/40 Iteration: 7560 | Train loss: 0.01033\n",
      "Epoch: 31/40 Iteration: 7580 | Train loss: 0.08039\n",
      "Epoch: 31/40 Iteration: 7600 | Train loss: 0.06011\n",
      "Epoch: 31/40 Iteration: 7620 | Train loss: 0.02885\n",
      "Epoch: 31/40 Iteration: 7640 | Train loss: 0.00695\n",
      "Epoch: 31/40 Iteration: 7660 | Train loss: 0.02186\n",
      "Epoch: 31/40 Iteration: 7680 | Train loss: 0.01064\n",
      "Epoch: 31/40 Iteration: 7700 | Train loss: 0.00747\n",
      "Epoch: 31/40 Iteration: 7720 | Train loss: 0.00359\n",
      "Epoch: 31/40 Iteration: 7740 | Train loss: 0.02679\n",
      "Epoch: 32/40 Iteration: 7760 | Train loss: 0.05298\n",
      "Epoch: 32/40 Iteration: 7780 | Train loss: 0.03298\n",
      "Epoch: 32/40 Iteration: 7800 | Train loss: 0.01773\n",
      "Epoch: 32/40 Iteration: 7820 | Train loss: 0.01805\n",
      "Epoch: 32/40 Iteration: 7840 | Train loss: 0.00774\n",
      "Epoch: 32/40 Iteration: 7860 | Train loss: 0.03015\n",
      "Epoch: 32/40 Iteration: 7880 | Train loss: 0.05951\n",
      "Epoch: 32/40 Iteration: 7900 | Train loss: 0.07349\n",
      "Epoch: 32/40 Iteration: 7920 | Train loss: 0.08576\n",
      "Epoch: 32/40 Iteration: 7940 | Train loss: 0.02442\n",
      "Epoch: 32/40 Iteration: 7960 | Train loss: 0.03349\n",
      "Epoch: 32/40 Iteration: 7980 | Train loss: 0.01830\n",
      "Epoch: 32/40 Iteration: 8000 | Train loss: 0.04210\n",
      "Epoch: 33/40 Iteration: 8020 | Train loss: 0.12387\n",
      "Epoch: 33/40 Iteration: 8040 | Train loss: 0.01806\n",
      "Epoch: 33/40 Iteration: 8060 | Train loss: 0.00623\n",
      "Epoch: 33/40 Iteration: 8080 | Train loss: 0.07399\n",
      "Epoch: 33/40 Iteration: 8100 | Train loss: 0.06588\n",
      "Epoch: 33/40 Iteration: 8120 | Train loss: 0.02366\n",
      "Epoch: 33/40 Iteration: 8140 | Train loss: 0.00467\n",
      "Epoch: 33/40 Iteration: 8160 | Train loss: 0.00880\n",
      "Epoch: 33/40 Iteration: 8180 | Train loss: 0.00546\n",
      "Epoch: 33/40 Iteration: 8200 | Train loss: 0.00853\n",
      "Epoch: 33/40 Iteration: 8220 | Train loss: 0.01031\n",
      "Epoch: 33/40 Iteration: 8240 | Train loss: 0.02669\n",
      "Epoch: 34/40 Iteration: 8260 | Train loss: 0.00844\n",
      "Epoch: 34/40 Iteration: 8280 | Train loss: 0.12102\n",
      "Epoch: 34/40 Iteration: 8300 | Train loss: 0.01764\n",
      "Epoch: 34/40 Iteration: 8320 | Train loss: 0.00872\n",
      "Epoch: 34/40 Iteration: 8340 | Train loss: 0.00571\n",
      "Epoch: 34/40 Iteration: 8360 | Train loss: 0.01043\n",
      "Epoch: 34/40 Iteration: 8380 | Train loss: 0.06110\n",
      "Epoch: 34/40 Iteration: 8400 | Train loss: 0.04544\n",
      "Epoch: 34/40 Iteration: 8420 | Train loss: 0.09893\n",
      "Epoch: 34/40 Iteration: 8440 | Train loss: 0.01251\n",
      "Epoch: 34/40 Iteration: 8460 | Train loss: 0.03301\n",
      "Epoch: 34/40 Iteration: 8480 | Train loss: 0.00191\n",
      "Epoch: 34/40 Iteration: 8500 | Train loss: 0.00857\n",
      "Epoch: 35/40 Iteration: 8520 | Train loss: 0.00885\n",
      "Epoch: 35/40 Iteration: 8540 | Train loss: 0.01106\n",
      "Epoch: 35/40 Iteration: 8560 | Train loss: 0.07105\n",
      "Epoch: 35/40 Iteration: 8580 | Train loss: 0.10744\n",
      "Epoch: 35/40 Iteration: 8600 | Train loss: 0.05929\n",
      "Epoch: 35/40 Iteration: 8620 | Train loss: 0.02696\n",
      "Epoch: 35/40 Iteration: 8640 | Train loss: 0.00555\n",
      "Epoch: 35/40 Iteration: 8660 | Train loss: 0.00889\n",
      "Epoch: 35/40 Iteration: 8680 | Train loss: 0.01261\n",
      "Epoch: 35/40 Iteration: 8700 | Train loss: 0.00800\n",
      "Epoch: 35/40 Iteration: 8720 | Train loss: 0.00212\n",
      "Epoch: 35/40 Iteration: 8740 | Train loss: 0.01426\n",
      "Epoch: 36/40 Iteration: 8760 | Train loss: 0.00407\n",
      "Epoch: 36/40 Iteration: 8780 | Train loss: 0.00552\n",
      "Epoch: 36/40 Iteration: 8800 | Train loss: 0.01961\n",
      "Epoch: 36/40 Iteration: 8820 | Train loss: 0.00728\n",
      "Epoch: 36/40 Iteration: 8840 | Train loss: 0.00597\n",
      "Epoch: 36/40 Iteration: 8860 | Train loss: 0.00768\n",
      "Epoch: 36/40 Iteration: 8880 | Train loss: 0.06750\n",
      "Epoch: 36/40 Iteration: 8900 | Train loss: 0.01913\n",
      "Epoch: 36/40 Iteration: 8920 | Train loss: 0.07946\n",
      "Epoch: 36/40 Iteration: 8940 | Train loss: 0.00861\n",
      "Epoch: 36/40 Iteration: 8960 | Train loss: 0.01935\n",
      "Epoch: 36/40 Iteration: 8980 | Train loss: 0.00358\n",
      "Epoch: 36/40 Iteration: 9000 | Train loss: 0.00825\n",
      "Epoch: 37/40 Iteration: 9020 | Train loss: 0.00830\n",
      "Epoch: 37/40 Iteration: 9040 | Train loss: 0.00373\n",
      "Epoch: 37/40 Iteration: 9060 | Train loss: 0.00244\n",
      "Epoch: 37/40 Iteration: 9080 | Train loss: 0.04137\n",
      "Epoch: 37/40 Iteration: 9100 | Train loss: 0.04512\n",
      "Epoch: 37/40 Iteration: 9120 | Train loss: 0.00370\n",
      "Epoch: 37/40 Iteration: 9140 | Train loss: 0.00253\n",
      "Epoch: 37/40 Iteration: 9160 | Train loss: 0.00384\n",
      "Epoch: 37/40 Iteration: 9180 | Train loss: 0.00269\n",
      "Epoch: 37/40 Iteration: 9200 | Train loss: 0.00269\n",
      "Epoch: 37/40 Iteration: 9220 | Train loss: 0.00224\n",
      "Epoch: 37/40 Iteration: 9240 | Train loss: 0.00848\n",
      "Epoch: 38/40 Iteration: 9260 | Train loss: 0.00264\n",
      "Epoch: 38/40 Iteration: 9280 | Train loss: 0.00181\n",
      "Epoch: 38/40 Iteration: 9300 | Train loss: 0.01484\n",
      "Epoch: 38/40 Iteration: 9320 | Train loss: 0.00376\n",
      "Epoch: 38/40 Iteration: 9340 | Train loss: 0.00687\n",
      "Epoch: 38/40 Iteration: 9360 | Train loss: 0.00323\n",
      "Epoch: 38/40 Iteration: 9380 | Train loss: 0.07419\n",
      "Epoch: 38/40 Iteration: 9400 | Train loss: 0.00523\n",
      "Epoch: 38/40 Iteration: 9420 | Train loss: 0.06449\n",
      "Epoch: 38/40 Iteration: 9440 | Train loss: 0.00411\n",
      "Epoch: 38/40 Iteration: 9460 | Train loss: 0.01855\n",
      "Epoch: 38/40 Iteration: 9480 | Train loss: 0.00225\n",
      "Epoch: 38/40 Iteration: 9500 | Train loss: 0.00463\n",
      "Epoch: 39/40 Iteration: 9520 | Train loss: 0.00707\n",
      "Epoch: 39/40 Iteration: 9540 | Train loss: 0.00186\n",
      "Epoch: 39/40 Iteration: 9560 | Train loss: 0.00199\n",
      "Epoch: 39/40 Iteration: 9580 | Train loss: 0.02865\n",
      "Epoch: 39/40 Iteration: 9600 | Train loss: 0.05547\n",
      "Epoch: 39/40 Iteration: 9620 | Train loss: 0.00698\n",
      "Epoch: 39/40 Iteration: 9640 | Train loss: 0.00470\n",
      "Epoch: 39/40 Iteration: 9660 | Train loss: 0.00422\n",
      "Epoch: 39/40 Iteration: 9680 | Train loss: 0.00173\n",
      "Epoch: 39/40 Iteration: 9700 | Train loss: 0.00315\n",
      "Epoch: 39/40 Iteration: 9720 | Train loss: 0.00101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/40 Iteration: 9740 | Train loss: 0.00269\n",
      "Epoch: 40/40 Iteration: 9760 | Train loss: 0.00222\n",
      "Epoch: 40/40 Iteration: 9780 | Train loss: 0.00129\n",
      "Epoch: 40/40 Iteration: 9800 | Train loss: 0.00473\n",
      "Epoch: 40/40 Iteration: 9820 | Train loss: 0.00124\n",
      "Epoch: 40/40 Iteration: 9840 | Train loss: 0.00367\n",
      "Epoch: 40/40 Iteration: 9860 | Train loss: 0.00754\n",
      "Epoch: 40/40 Iteration: 9880 | Train loss: 0.05668\n",
      "Epoch: 40/40 Iteration: 9900 | Train loss: 0.00582\n",
      "Epoch: 40/40 Iteration: 9920 | Train loss: 0.06730\n",
      "Epoch: 40/40 Iteration: 9940 | Train loss: 0.00260\n",
      "Epoch: 40/40 Iteration: 9960 | Train loss: 0.03897\n",
      "Epoch: 40/40 Iteration: 9980 | Train loss: 0.00242\n",
      "Epoch: 40/40 Iteration: 10000 | Train loss: 0.00527\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model/sentiment-39.ckpt\n",
      "Test Acc.: 0.841\n"
     ]
    }
   ],
   "source": [
    "## Test: \n",
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (\n",
    "      np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/sentiment-39.ckpt\n"
     ]
    }
   ],
   "source": [
    "## Get probabilities:\n",
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6822090e-06, 9.9998462e-01, 8.2683563e-04, ..., 1.7203720e-06,\n",
       "       1.8015897e-02, 8.8451606e-01], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
