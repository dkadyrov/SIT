{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.4 64-bit",
   "display_name": "Python 3.8.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Daniel Kadyrov\n",
    "\n",
    "Stevens ID: 10455680\n",
    "\n",
    "CS557 - Natural Language Processing\n"
   ]
  },
  {
   "source": [
    "## Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "\n",
    "Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2), in terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and so on for bigrams and unigrams.\n",
    "\n",
    "$ P(w_3 | w_1, w_2) = \\frac{C(w1, w2, w3) +k}{C(w_1, w_2) + kV} = \\frac{C(w_1, w_2, w_3) + 1}{C(w_1, w_2)+V}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 3.8\n",
    "\n",
    "Write a program to compute unsmoothed unigrams and bigrams."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"My name is Daniel Kadyrov and this is my Assignment 2\"\n",
    "\n",
    "unigram = word_tokenize(text)\n",
    "unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import bigrams\n",
    "\n",
    "list(bigrams(unigram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "syn_arr = wn.synsets('room')\n",
    "\n",
    "for word in syn_arr: \n",
    "    print(word)"
   ]
  },
  {
   "source": [
    "### Exercise 4.3\n",
    "\n",
    "Train two models, multinominal naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted. Use both naive Bayes models to assign a class (pos or neg) to this sentence: A good, good plot and great characters, but poor acting.\n",
    "\n",
    "Do the two models agree or disagree?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# text = [\"A good, good plot and great characterrs, but poor acting.\", []]\n",
    "\n",
    "token = [\"good \", \"poor \", \"great \"]\n",
    "\n",
    "# Generate Documents\n",
    "counts = [\n",
    "    [3, 0, 3],\n",
    "    [0, 1, 2],\n",
    "    [1, 3, 0],  \n",
    "    [1, 5, 2],\n",
    "    [0, 2, 0]\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for doc in counts: \n",
    "    text = \"\"\n",
    "    for c in range(len(doc)): \n",
    "        text += token[c]* doc[c]\n",
    "    docs.append(text)\n",
    "\n",
    "Y = [0, 0, 1, 1, 1] # 0: pos, 1: neg\n",
    "\n",
    "cls = {\n",
    "    0: \"pos\",\n",
    "    1: \"neg\"\n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "X_test = vectorizer.transform([\"A good, good plot and great characterrs, but poor acting.\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'pos'"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X, Y)\n",
    "MNB_result = MNB.predict(X_test)\n",
    "cls[MNB_result[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'neg'"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X, Y)\n",
    "BNB_result = BNB.predict(X_test)\n",
    "cls[BNB_result[0]]"
   ]
  },
  {
   "source": [
    "The models disagree. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Your Turn P69\n",
    "Your Turn: Write down all the senses of the word dish that you can think of. Now, explore this word with the help of WordNet, using the same operations shown earlier.\n",
    "\n",
    "dish, dishsoap, dishwasher, dishwasher, plate, disc, satelite, bowl "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('dish'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "source": [
    "### Your Turn P70\n",
    "\n",
    "Your Turn: Try out NLTKâ€™s convenient graphical WordNet browser: nltk.app.wordnet(). Explore the WordNet hierarchy by following the hypernym and hyponym links."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.app.wordnet()"
   ]
  },
  {
   "source": [
    "### Exercise 2.8.5\n",
    "\n",
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use member_mer onyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms().\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fork = wn.synsets(\"fork\")[0]\n",
    "print(fork.member_meronyms())\n",
    "print(fork.part_meronyms())\n",
    "print(fork.substance_meronyms())\n",
    "print(fork.member_holonyms())\n",
    "print(fork.part_holonyms())\n",
    "print(fork.substance_holonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prong = wn.synsets(\"prong\")[0]\n",
    "print(prong.member_meronyms())\n",
    "print(prong.part_meronyms())\n",
    "print(prong.substance_meronyms())\n",
    "print(prong.member_holonyms())\n",
    "print(prong.part_holonyms())\n",
    "print(prong.substance_holonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "buckle = wn.synsets(\"buckle\")[0]\n",
    "print(buckle.member_meronyms())\n",
    "print(buckle.part_meronyms())\n",
    "print(buckle.substance_meronyms())\n",
    "print(buckle.member_holonyms())\n",
    "print(buckle.part_holonyms())\n",
    "print(buckle.substance_holonyms())"
   ]
  },
  {
   "source": [
    "### Exercise 2.8.14\n",
    "\n",
    "Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def supergloss(s): \n",
    "    synset = [s.name, s.definition] \n",
    "    hypernyms = s.hypernyms()\n",
    "    hyponyms = s.hyponyms()\n",
    "\n",
    "    if hypernyms != []:\n",
    "        for n in range(len(hypernyms)): \n",
    "            for hypernym in s.hypernyms(): \n",
    "                hypernyms[n] = [hypernym.name(), hypernym.definition()]\n",
    "    else:\n",
    "        hypernym = None\n",
    "\n",
    "    if hyponyms != []: \n",
    "        for n in range(len(hyponyms)): \n",
    "            for hyponym in s.hyponyms():\n",
    "                hyponyms[n] = [hyponym.name(), hyponym.definition()]\n",
    "    else:\n",
    "        hyponyms = None\n",
    "\n",
    "    result = \"root: {} \\n, hypernyms: {} \\n, hyponyms: {} \\n\".format(s.name(), hypernyms, hyponyms)\n",
    "\n",
    "    return result\n",
    "\n",
    "test = wn.synsets(\"test\")[0]\n",
    "supergloss(test)\n"
   ]
  },
  {
   "source": [
    "### Exercize 2.8.27 \n",
    "\n",
    "The polysemy of a word is the number of senses it has. Using WordNet, we can determine that the noun dog has seven senses with len(wn.synsets('dog', 'n')). Compute the average polysemy of nouns, verbs, adjectives, and adverbs according to WordNet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word = \"cram\"\n",
    "\n",
    "for synset in wn.synsets(word):\n",
    "    offset = str(synset.offset()).zfill(8)\n",
    "    pos = synset.pos()\n",
    "    idx = \"{} {}\".format(offset, pos)\n",
    "    d = synset.definition()\n",
    "    lemmas = synset.lemma_names()\n",
    "\n",
    "    print(\"{}: {} ({})\".format(idx, d, \", \".join(lemmas)))"
   ]
  },
  {
   "source": [
    "## Part 3 \n",
    "\n",
    "Find longest sentence in the Inaugural Address Corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "max_len = 0\n",
    "max_sent = \"\"\n",
    "\n",
    "for sentance in inaugural.sents(): \n",
    "    if len(sentance) > max_len:\n",
    "        max_len = len(sentance)\n",
    "        max_sent = sentance\n",
    "\n",
    "print(max_len)\n",
    "print(\" \".join(max_sent))"
   ]
  },
  {
   "source": [
    "## Part 4\n",
    "\n",
    "Perplexity measures the prediction ability of a probability model on a text. Lexical Diversity measures the ratrio of unique words to the total number of words in a text. The more diverse a text document is, the more distribution of words the probability model would have to train. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE \n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(2, inaugural.raw())\n",
    "\n",
    "lm = MLE(2)\n",
    "lm.fit(train, vocab)\n",
    "lm.perplexity(inaugural.raw())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(text) / len(set(text))\n",
    "\n",
    "lexical_diversity(inaugural.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}