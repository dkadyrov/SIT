{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Daniel Kadyrov\n",
    "\n",
    "Stevens ID: 10455680\n",
    "\n",
    "CS557 - Natural Language Processing \n",
    "\n",
    "Group 32 - Daniel Kadyrov\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 1\n",
    "\n",
    "Apply logit regression to the 2-category data in RFMdataMPJ.xlsx. Preview the document or the Iris data (Links to an external site.) with multiple categories.\n",
    "\n",
    "Python has logistic regression capability; try the sklearn package. You are welcome to use any other, but in any case, give a reference for it. Or you are welcome to work through the Building A Logistic Regression in Python, Step by Step (Links to an external site.) tutorial and apply it to the data in the RFM and/or Iris file.\n",
    "\n",
    "The Iris data describes pedal and sepal length and width for 150 Irises classified into three categories. The pedal and sepal length and width are four features to be used to train a classifier. You might want to hold out at least three Irises (one from each category) to be classified by the probability of them being in the categories and see how accurate the classifier is. When applied to NLP classification you can associate the features of some text categories with their classification in a similar manner. For pedagogic purposes the 4-feature Iris dataset is suggested since text features often tend to be much larger, such as many of the most frequent words, proper names, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8666666666666667"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "source": [
    "# Part 2 \n",
    "\n",
    "Like in the previous Assignment, train the sentiment classifier using logistic regression, but follow the directions in this paragraph. Use a corpus of tagged reviews similar to the way Amazon does it. Each review is given a tag on a scale, say, from 1 to 5, the higher the tag the more favorable the review, or as categories like favorable, unfavorable, neutral. A 5-point category scale can be very favorable, favorable, neutral, unfavorable, and very unfavorable. In any case, treat the ratings as a set of categories, not numerical values. Find files, one for favorable (Links to an external site.) words and one for unfavorable (Links to an external site.) ones, and count the number of each in each review. Also count the numbers of words that change these into the other category, like “not” and “hardly” as in “hardly amusing”, the “un-“ prefix as in “unhappy”, and the “a-“ prefix as in “amoral”, and any other negators you can think off or find. Regular expressions are useful for this. Lists of these may be more difficult to find so you may have to make some up. These counts become the features (independent variables) used as inputs to the logistic regression model. Each review has three independent variables/features: count of favorable words, count of unfavorable words, and the count of negators, and one dependent variable, the category with the highest probability. Then report a confusion matrix (example in Course Materials) to show how accurate the model matches the training set categories. If you get enough data create the training, validation, and testing partitions and use the matrix functions in Excel, the xla Excel add-in, or a matrix algebra package like MATLAB or TensorFlow.  Show your results in a confusion matrix.  For 2 categories you can use ClassifyConfusionMatrix.xlsxPreview the document., for more than two categories check here (Links to an external site.)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = [] \n",
    "\n",
    "with open(\"data/opinion-lexicon-English/positive-words.txt\", \"r\", encoding=\"ISO-8859-1\") as file: \n",
    "    lines = [l for l in (line.strip() for line in file) if l]\n",
    "    for line in lines: \n",
    "        if \";\" in line:\n",
    "            pass\n",
    "        else:\n",
    "            positive.append(line)\n",
    "\n",
    "with open(\"data/opinion-lexicon-English/negative-words.txt\", \"r\", encoding=\"ISO-8859-1\") as file: \n",
    "    lines = [l for l in (line.strip() for line in file) if l]\n",
    "    for line in lines: \n",
    "        if \";\" in line:\n",
    "            pass\n",
    "        else:\n",
    "            negative.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"data/processed_stars/books/all_balanced.review\", \"r\") as file: \n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for line in file:\n",
    "        review = []\n",
    "        liner = line.split(\" \") \n",
    "        for word in liner:\n",
    "            if \"#label#\" in word: \n",
    "                labels.append(int(float(word.split(\":\")[-1].split()[0])))\n",
    "            else:\n",
    "                review.append(word.split(\":\")[0]+\" \" * int(word.split(\":\")[1]))\n",
    "\n",
    "        reviews.append(\" \".join(review).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "negator = [\"un\", \"not\", \"hardly\", \"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = [] \n",
    "for review in reviews: \n",
    "    p = 0\n",
    "    n = 0 \n",
    "    nn = 0\n",
    "    for word in review: \n",
    "        if word in positive: \n",
    "            p += 1 \n",
    "        if word in negative: \n",
    "            n += 1\n",
    "        if word in negator: \n",
    "            nn += 1 \n",
    "        for neg in negator: \n",
    "            if neg in word: \n",
    "                nn += 1\n",
    "        if word.startswith(\"a\"):\n",
    "            nn += 1\n",
    "\n",
    "    X.append([p, n, nn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3851744186046512"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train)\n",
    "\n",
    "predict = LR.predict(x_test)\n",
    "\n",
    "LR.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[211  53  34  56]\n [155  68  50  71]\n [ 61  61  50 165]\n [ 64  34  42 201]]\n"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predict)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "source": [
    "# Part 3\n",
    "\n",
    "Demonstrate embeddings for a small vocabulary and some phrases/sentences. Check out word2vec_basic.py in the gensim package and similar resources. BKL may not be helpful for this but check out the online Python 3 version anyway. There are demos on line (Links to an external site.). Report the confusion matrix for the application of the model to the test set.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(reviews, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('guess_i', 0.9869878888130188),\n ('feeling_that', 0.9866390228271484),\n ('good_writing', 0.9859217405319214),\n ('consider_it', 0.985460638999939),\n ('and_as', 0.985217809677124),\n ('crime_and', 0.9851363897323608),\n ('army', 0.9847644567489624),\n ('are_better', 0.9845976829528809),\n ('and_am', 0.9845890402793884),\n ('too_complex', 0.9843344688415527)]"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "w1 = [\"dirty\"]\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('ago_the', 0.9930318593978882),\n ('he_turned', 0.9921306371688843),\n ('but_unfortunately', 0.9920940399169922),\n ('were_written', 0.9916591644287109),\n ('presented_and', 0.9908245801925659),\n ('then_a', 0.9907580614089966),\n ('possibly', 0.9906696081161499),\n ('cents', 0.9905273914337158),\n ('and_background', 0.9904022216796875),\n ('hopefully', 0.9903094172477722)]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "w2 = [\"broken\"]\n",
    "model.wv.most_similar(positive=w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_positive = []\n",
    "for pos in positive: \n",
    "    try: \n",
    "        new_positive.append(model.most_similar(pos)[0][0])\n",
    "    except:\n",
    "        continue\n",
    "positive += new_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_negative = []\n",
    "for neg in negative: \n",
    "    try: \n",
    "        new_negative.append(model.most_similar(neg)[0][0])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "negative += new_negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_negator= []\n",
    "for neg in negator: \n",
    "    try: \n",
    "        new_negator.append(model.most_similar(neg)[0][0])\n",
    "    except:\n",
    "        pass\n",
    "negator += new_negator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for review in reviews: \n",
    "    p = 0\n",
    "    n = 0 \n",
    "    nn = 0\n",
    "    for word in review: \n",
    "        if word in positive: \n",
    "            p += 1 \n",
    "        if word in negative: \n",
    "            n += 1\n",
    "        if word in negator: \n",
    "            nn += 1 \n",
    "        for neg in negator: \n",
    "            if neg in word: \n",
    "                nn += 1\n",
    "        if word.startswith(\"a\"):\n",
    "            nn += 1\n",
    "\n",
    "    X.append([p, n, nn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.37281976744186046"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=0)\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train)\n",
    "\n",
    "predict = LR.predict(x_test)\n",
    "\n",
    "LR.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[217,  39,  34,  64],\n       [175,  44,  51,  74],\n       [ 81,  40,  54, 162],\n       [ 69,  40,  34, 198]])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predict)"
   ]
  },
  {
   "source": [
    "# Part 4 \n",
    "\n",
    "Examine the methods, such as document embeddings, in A&S Ch4 from p83 on applied to sentence tokenizers. Compare them to those BKL sections on processing sentences. This is an exploratory assignment which could be open ended and possibly used as a final project. Do not get bogged down.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Sentence tokenization involves segementing a document into sentences. The NLTK package provides the function sent_tokenize() to seperate a document into a list/array of sentences [NLTK Sentence Tokenize](https://www.nltk.org/api/nltk.tokenize.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}