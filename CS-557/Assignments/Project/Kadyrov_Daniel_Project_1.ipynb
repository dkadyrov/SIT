{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Daniel Kadyrov\n",
    "\n",
    "Stevens ID: 10455680\n",
    "\n",
    "CS557 - Natural Language Processing\n",
    "\n",
    "Group 32 - Daniel Kadyrov"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 1\n",
    "\n",
    "Respond to any one of Chapter 5, 6, 7, or 8 Exercises of BKL. Report on the packages/programs used, to what it/they were applied, the results, and your interpretation.\n",
    "\n",
    "## Exercise 6.10\n",
    "\n",
    "Suppose you wanted to automatically generate a prose description of a scene, and already had a word to uniquely describe each entity, such as the book, and simply wanted to decide whether to use in or on in relating various items, e.g., the book is in the cupboard versus the book is on the shelf. Explore this issue by looking at corpus data and writing programs as needed. Consider the following examples:\n",
    "\n",
    "a. in the car versus on the train\n",
    "\n",
    "b. in town versus on campus\n",
    "\n",
    "c. in the picture versus on the screen\n",
    "\n",
    "d. in Macbeth versus on Letterman\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This was a problem that spoke to me as a New Yorker because of the amount of \"on vs. in\" arguements I have with friends from neighboring locations. Are you in line or on line? are you in NYC or on Long Island? My understanding of this problem, based on this chapter, is to generate a database of uses for \"on\" vs \"in\". I used the NLTK Brown corpus.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "in the car: 9\non the train: 8\nin town: 21\non campus: 1\nin the picture: 5\non the screen: 6\nin Macbeth: 1\non Letterman: 0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = brown.tagged_sents()\n",
    "\n",
    "sents = [\n",
    "    \"in the car\",\n",
    "    \"on the train\",\n",
    "    \"in town\",\n",
    "    \"on campus\",\n",
    "    \"in the picture\",\n",
    "    \"on the screen\",\n",
    "    \"in Macbeth\",\n",
    "    \"on Letterman\"\n",
    "]\n",
    "\n",
    "db = {} \n",
    "for sent in sents:\n",
    "    db[sent] = [] \n",
    "    flag = False\n",
    "\n",
    "    sent_tok = word_tokenize(sent)\n",
    "\n",
    "    length = len(sent_tok)\n",
    "    for s in corpus: \n",
    "        for i in range(len(s) - length):\n",
    "            for j in range(length):  \n",
    "                if s[i+j][0] == sent_tok[j]:\n",
    "                    flag = True\n",
    "                else:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag: \n",
    "                db[sent].append(s)\n",
    "\n",
    "for key in db.keys(): \n",
    "    print(\"{}: {}\".format(key, len(db[key])))\n"
   ]
  },
  {
   "source": [
    "# Part 2\n",
    "## Exercise 42\n",
    "Use WordNet to create a semantic index for a text collection. Extend the concordance search program in Example 3-1, indexing each word using the offset of its first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some of its ancestors in the hypernym hierarchy)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "def semantic_index(text, ancestors=False):\n",
    "    stopword = stopwords.words(\"english\")\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    array = []\n",
    "\n",
    "    for t in text.split():\n",
    "        t_lemma = lemmatizer.lemmatize(t.lower())\n",
    "\n",
    "        if t_lemma not in stopword:\n",
    "            try: \n",
    "                index = str(wordnet.synsets(t_lemma)[0].offset())\n",
    "                index += wordnet.synsets(t_lemma)[0].pos()\n",
    "\n",
    "                if ancestors == True: \n",
    "                    hp = str(wordnet.synsets(t_lemma)[0].hypernyms()[0].offset())\n",
    "                    hp += wordnet.synsets(t_lemma)[0].hypernyms()[0].pos()\n",
    "                    index = \" || \".join((index,hp))\n",
    "                \n",
    "                array.append(\" || \".join((t, index))) \n",
    "            except: \n",
    "                array.append(t)\n",
    "        else: \n",
    "            array.append(t)\n",
    "\n",
    "    return \" \".join(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'On this subject || 6599788n it might || 5030680n become || 149583v me better || 5143558n to be silent || 1919428s or to speak || 941990v with diffidence; but as something may || 15211484n be expected, the occasion, I hope, will be admitted || 817311v as an apology || 6633363n if I venture || 797878n to say || 14485526n that if a preference, upon principle, of a free || 7947958n republican || 10522633n government, formed || 2448185v upon long || 1828405v and serious || 2118379a reflection, after a diligent || 1736122s and impartial || 1723308a inquiry || 5797597n after truth; if an attachment || 7545161n to the Constitution || 6533648n of the United || 2469835v States, and a conscientious || 310138s determination || 151497n to support || 1215902n it until it shall be altered || 126264v by the judgments || 5837957n and wishes || 7486229n of the people, expressed || 943837v in the mode || 4928903n prescribed || 747135v in it; if a respectful || 1993940a attention || 5702275n to the constitutions || 6533648n of the individual || 7846n States || 8654360n and a constant || 5858936n caution || 4664058n and delicacy || 4813066n toward the State || 8654360n governments; if an equal || 9626238n and impartial || 1723308a regard || 5820170n to the rights, interest, honor, and happiness || 13987423n of all the States || 8654360n in the Union, without preference || 7498210n or regard || 5820170n to a northern || 6949207n or southern, an eastern || 823971s or western, position, their various || 2065665s political || 1814385a opinions || 5945642n on unessential || 902652a points || 5865998n or their personal || 6271288n attachments; if a love || 7543288n of virtuous || 2513269a men || 8212347n of all parties || 8256968n and denominations; if a love || 7543288n of science || 5999797n and letters || 6624161n and a wish || 7486229n to patronize || 2219940v every || 2269794s rational || 13730469n effort || 786195n to encourage || 2554922v schools, colleges, universities, academies, and every || 2269794s institution || 8053576n for propagating || 2230447v knowledge, virtue, and religion || 5946687n among all classes || 7997703n of the people, not only for their benign || 2594565a influence || 5194151n on the happiness || 13987423n of life || 13963192n in all its stages || 15290337n and classes, and of society || 7966140n in all its forms, but as the only means || 6023969n of preserving || 2679899v our Constitution || 6533648n from its natural || 10346392n enemies, the spirit || 10636598n of sophistry, the spirit || 10636598n of party, the spirit || 10636598n of intrigue, the profligacy || 4894807n of corruption, and the pestilence || 14138691n of foreign || 1037540a influence, which is the angel || 9538915n of destruction || 217014n to elective || 890808n governments; if a love || 7543288n of equal || 9626238n laws, of justice, and humanity || 4829182n in the interior || 8588294n administration; if an inclination || 6196584n to improve || 205885v agriculture, commerce, and manufacturers || 8060446n for necessity, convenience, and defense; if a spirit || 10636598n of equity || 13333696n and humanity || 4829182n toward the aboriginal || 9676490n nations || 8168978n of America, and a disposition || 4623612n to meliorate || 205885v their condition || 13920835n by inclining || 335384n them to be more friendly || 8397489n to us, and our citizens || 9923673n to be more friendly || 8397489n to them; if an inflexible || 1024597a determination || 151497n to maintain || 2681795v peace || 13970236n and inviolable || 2510604a faith || 5946687n with all nations, and that system || 4377057n of neutrality || 1240850n and impartiality || 6202686n among the belligerent || 9939313n powers || 5190804n of Europe || 9275473n which has || 13888783n been adopted || 2346895v by this Government || 8050678n and so solemnly || 189960r sanctioned || 806502v by both Houses || 3544360n of Congress || 8161757n and applauded || 861929v by the legislatures || 8163273n of the States || 8654360n and the public || 8179689n opinion, until it shall be otherwise || 2071301s ordained || 2427916v by Congress; if a personal || 6271288n esteem || 14437552n for the French || 6964901n nation, formed || 2448185v in a residence || 8558963n of seven || 13744916n years || 15203791n chiefly || 73897r among them, and a sincere || 2179279a desire || 7484265n to preserve || 14515463n the friendship || 13931145n which has || 13888783n been so much || 13776621n for the honor || 6696483n and interest || 5682950n of both nations; if, while the conscious || 1337767s honor || 6696483n and integrity || 14460565n of the people || 7942152n of America || 9044862n and the internal || 948670a sentiment || 7481951n of their own power || 5190804n and energies || 11452218n must || 9363970n be preserved, an earnest || 13350182n endeavor || 796886n to investigate || 789138v every || 2269794s just cause || 7326557n and remove || 5090255n every || 2269794s colorable pretense || 754956n of complaint; if an intention || 5982152n to pursue || 2375131v by amicable || 1246579a negotiation || 7148192n a reparation || 13292613n for the injuries || 14285662n that have been committed || 2582615v on the commerce || 1090446n of our fellow-citizens by whatever || 2267686s nation, and if success || 7319103n can not be obtained, to lay || 7049713n the facts || 5817396n before the Legislature, that they may || 15211484n consider || 690614v what further measures || 174412n the honor || 6696483n and interest || 5682950n of the Government || 8050678n and its constituents || 3081021n demand; if a resolution || 6511874n to do justice || 4850117n as far || 8016900n as may || 15211484n depend || 2664234v upon me, at all times || 7309599n and to all nations, and maintain || 2681795v peace, friendship, and benevolence || 7545717n with all the world; if an unshaken || 1991783s confidence || 5697363n in the honor, spirit, and resources || 13331778n of the American || 9738708n people, on which I have so often || 35058r hazarded || 916909v my all and never || 20759r been deceived; if elevated || 3280813n ideas || 5833840n of the high || 5097536n destinies || 7330007n of this country || 8168978n and of my own duties || 1129920n toward it, founded || 2427103v on a knowledge || 23271n of the moral || 6606044n principles || 5913538n and intellectual || 9621545n improvements || 7357388n of the people || 7942152n deeply || 173353r engraven on my mind || 5611302n in early || 812952a life, and not obscured || 2157731v but exalted || 860620v by experience || 5758059n and age; and, with humble || 1801697v reverence, I feel || 5677340n it to be my duty || 1129920n to add, if a veneration || 7521039n for the religion || 5946687n of a people || 7942152n who profess || 819756v and call || 6272803n themselves Christians, and a fixed || 260648v resolution || 6511874n to consider || 690614v a decent || 1993408s respect || 5820170n for Christianity || 6226057n among the best || 127531n recommendations || 6671637n for the public || 8179689n service, can enable || 512877v me in any degree || 5093890n to comply || 2542280v with your wishes, it shall be my strenuous || 875235s endeavor || 796886n that this sagacious || 2569558s injunction || 7170467n of the two || 13743269n Houses || 3544360n shall not be without effect.'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sents = sent_tokenize(inaugural.raw())\n",
    "max_sent = max(sents, key=len)\n",
    "\n",
    "semantic_index(max_sent)"
   ]
  },
  {
   "source": [
    "# Part 3\n",
    "\n",
    "## Exercise 11.3 \n",
    "Rewrite the CKY algorithm given in Fig. 11.5 on page 228 so that it can accept\n",
    "grammars that contain unit productions.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The CKY algorithm is expanded so when a symbol is added as a cell to the table, all symbols that produced the original symbol are added. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[['ART'], [], [], ['S']],\n",
       " [[], ['ADJ'], [], ['X']],\n",
       " [[], [], ['ADJ'], ['NP']],\n",
       " [[], [], [], ['NOUN']]]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from cky import parser \n",
    "\n",
    "grammar_file = \"cky\\grammar.txt\"\n",
    "grammar = parser.Grammar([line.rstrip('\\n') for line in open(grammar_file)])\n",
    "\n",
    "sentence = \"the beautiful tropical fish\"\n",
    "\n",
    "\n",
    "parser.parse(sentence, grammar)"
   ]
  },
  {
   "source": [
    "## Exercise 11.4\n",
    "\n",
    "Discuss the relative advantages and disadvantages of partial versus full parsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Partial parsing is faster than full parsing but provides lower quality of  syntactical details. When a great amount of syntactic details is needed - a full parser performes better but requires more time. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exercise 11.5 \n",
    "\n",
    "Discuss how to augment a parser to deal with input that may be incorrect, for\n",
    "example, containing spelling errors or mistakes arising from automatic speech\n",
    "recognition."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Take partial syntactic structures that the parser can identify and combine them together. These new combined parses would introduce new rules."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 4\n",
    "\n",
    "Look up Latent Dirichlet Allocation (LDA), a method to define and discover topics in text, which is a generalization of sentence parsing, and find a Python package or program that implements it. Choose a short document (like a research paper or blog) to use as a training documents and two test documents, one with topics like the training document and one rather different. Run the results of the training documents and determine if they correctly identify the test documents as being similar or not. Interpret the result. It is possible this may not be able to be done in the rest of the semester; if so, describe what you learned about LDA and what you managed to do."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tika import parser \n",
    "\n",
    "training = parser.from_file(\"data\\Stevens Drone Detection Acoustic System.pdf\")[\"content\"]\n",
    "test1 = parser.from_file(\"data\\Long-term testing of acoustic system for tracking.pdf\")[\"content\"]\n",
    "test2 = parser.from_file(\"data\\A Playboy Interview With Miles Davis.pdf\")[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "article = Article(\"https://www.bbc.com/news/election-us-2020-55318269\")\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "training = article.text\n",
    "\n",
    "article = Article(\"https://www.aljazeera.com/news/2020/12/15/putin-finally-congratulates-biden-on-winning-us-presidency\")\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "test1 = article.text\n",
    "\n",
    "article = Article(\"https://www.foxnews.com/us/noreaster-suspends-outdoor-dining-new-york-city-after-snow-alert-issued\")\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "test2 = article.text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countVectorizer = CountVectorizer(stop_words=\"english\")\n",
    "termFrequency = countVectorizer.fit_transform([training])\n",
    "featureNames = countVectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "lda.fit(termFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic  0 working kind america approach arms biden bracing control develop differences\nTopic  1 russia working kind america approach arms biden bracing control develop\nTopic  2 working kind america approach arms biden bracing control develop differences\nTopic  3 working kind america approach arms biden bracing control develop differences\nTopic  4 working kind america approach arms biden bracing control develop differences\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(lda.components_):\n",
    "    print (\"Topic \", idx, \" \".join(featureNames[i] for i in topic.argsort()[:-10 - 1:-1]))"
   ]
  },
  {
   "source": [
    "Comparing the two similar articles shows a high similarity for Topic 1 (Russia)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00953391, 0.96186435, 0.00953391, 0.00953391, 0.00953391]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "lda.transform(countVectorizer.transform([test1]))"
   ]
  },
  {
   "source": [
    "Comparing the different topic article shows that they are not similar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.2, 0.2, 0.2, 0.2, 0.2]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "lda.transform(countVectorizer.transform([test2]))"
   ]
  },
  {
   "source": [
    "# Part 5\n",
    "J&M takes an academic approach to NLP involving theory, history, and various approaches with examples. Recently books and software have become available that allows users to proceed without such extensive knowledge by using Python packages whose authors have made many choices for the user, in effect claiming these are the best.  Two of these (by now there are likely many more, A&S might be one of them) taking this practical approach, as opposed to what they call the “bloated academic approach”, are listed below\n",
    "\n",
    "Thushan Ganegedara (2018) Natural Language Processing with TensorFlow, Packt Publishing Ltd., ISBN 978-1-78847-831-1\n",
    "\n",
    "Read Chapters 1-3, run some of the suggested code from Chapter 3, briefly explain what it is for, and describe and interpret its results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ganegadara's \"Natural Language Processing with TensorFlow\" provides great insight to topics discussed in this course and how to approach them using Python's powerful TensorFlow package. The benefit of TensorFlow is its ability to use the powerful GPU for processing. \n",
    "\n",
    "Chapter 3 of the book focuses on a nueral networks, a prominent feature of TensorFlow, for word representation with Word2Vec. Ganegadara demonstrates the use of TensorFlow for the skip-gram and Continuous Bag-of-Words algorithm. \n",
    "\n",
    "My only problem with this book is that it features incomplete and error filled code. Below, is the first code example in the chapter that then gets referenced in further sections. The code requires a function \"get_common_and_rare_word_ids\" however no where else in the book is this function defined, explained, or referenced. It is a little unbelievable that a paid book lacks this oversight. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_common_and_rare_word_ids' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7e913081ffea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Only pick dev samples in the head of the distribution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mvalid_window\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mvalid_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_common_and_rare_word_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_size\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_size\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mnum_sampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m \u001b[1;31m# Number of negative examples to sample.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_common_and_rare_word_ids' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "window_size = 4 # How many words to consider left and right.\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# Only pick dev samples in the head of the distribution.\n",
    "valid_window = 100\n",
    "valid_examples = get_common_and_rare_word_ids(valid_size//2,valid_size//2)\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=0.5 / math.sqrt(embedding_size)))\n",
    "softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled,num_classes=vocabulary_size))\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  }
 ]
}