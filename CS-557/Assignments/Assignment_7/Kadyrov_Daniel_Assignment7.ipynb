{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Daniel Kadyrov\n",
    "\n",
    "Stevens ID: 10455680\n",
    "\n",
    "CS557 - Natural Language Processing\n",
    "\n",
    "Group 32 - Daniel Kadyrov"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 1\n",
    "\n",
    "To get an idea about how difficult it is to unambiguously identify parts-of-speech select several words\n",
    "that can be a noun, verb, adjective, and adverb (see here (Links to an external site.) for instances),\n",
    "find their high frequency senses using WordNet,\n",
    "examine their definitions and example uses, and\n",
    "try to generate some rules for how to determine as to when they are a noun, verb, adjective, or adverb from their context."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"back\", \"best\", \"better\", \"bitter\", \"broadside\", \"clean\", \"clear\", \"close\", \"cod\", \"collect\", \"counter\", \"crisscross\", \"damn\", \"double\", \"down\", \"even\", \"express\", \"fair\", \"fast\", \"fine\", \"firm\", \"flush\", \"forward\", \"free\", \"full\", \"home\", \"jolly\", \"last\", \"light\", \"low\", \"o.k\", \"okay\", \"out\", \"pat\", \"plain\", \"plumb\", \"plump\", \"pop\", \"prompt\", \"quiet\", \"right\", \"rough\", \"round\", \"second\", \"short\", \"solo\", \"square\", \"steady\", \"still\", \"tiptoe\", \"true\", \"upstage\", \"well\", \"wholesale\", \"worst\", \"wrong\", \"zigzag\"]"
   ]
  },
  {
   "source": [
    "## Plump"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n\nthe sound of a sudden heavy fall\n['plump']\n[]\n[]\n[]\n[]\n----\nv\ndrop sharply\n['plummet', 'plump']\n[]\n[]\n[]\n[]\n----\nv\nset (something or oneself) down with or as if with a noise\n['plank', 'flump', 'plonk', 'plop', 'plunk', 'plump_down', 'plunk_down', 'plump']\n[]\n[]\n[]\n[]\n----\nv\nmake fat or plump\n['fatten', 'fat', 'flesh_out', 'fill_out', 'plump', 'plump_out', 'fatten_out', 'fatten_up']\n[Synset('feed.v.02')]\n[]\n[]\n[]\n----\nv\ngive support (to) or make a choice (of) one out of a group or number\n['plump', 'go']\n[]\n[]\n[]\n[]\n----\ns\nsufficiently fat so as to have a pleasing fullness of figure\n['chubby', 'embonpoint', 'plump']\n[]\n[]\n[]\n[]\n----\nr\nstraight down especially heavily or abruptly\n['plump']\n[]\n[]\n[]\n[]\n----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"plump\")\n",
    "\n",
    "for syn in syns: \n",
    "    print(syn.pos())\n",
    "    print(syn.definition())\n",
    "    print(syn.lemma_names())\n",
    "    print(syn.entailments())\n",
    "    print(syn.hyponyms())\n",
    "    print(syn.part_holonyms())\n",
    "    print(syn.part_meronyms())\n",
    "    # print(syn.frame_strings())\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'berries', 'were', 'plump', 'and', 'sweet']\n[]\n[Synset('berry.n.01'), Synset('berry.n.02'), Synset('berry.n.03'), Synset('berry.v.01')]\n[Synset('be.v.01'), Synset('be.v.02'), Synset('be.v.03'), Synset('exist.v.01'), Synset('be.v.05'), Synset('equal.v.01'), Synset('constitute.v.01'), Synset('be.v.08'), Synset('embody.v.02'), Synset('be.v.10'), Synset('be.v.11'), Synset('be.v.12'), Synset('cost.v.01')]\n[Synset('plump.n.01'), Synset('plummet.v.01'), Synset('plank.v.02'), Synset('fatten.v.01'), Synset('plump.v.04'), Synset('chubby.s.01'), Synset('plump.r.01')]\n[]\n[Synset('sweet.n.01'), Synset('dessert.n.01'), Synset('sweet.n.03'), Synset('sweet.n.04'), Synset('sweetness.n.02'), Synset('sweet.a.01'), Synset('angelic.s.03'), Synset('dulcet.s.02'), Synset('sweet.s.04'), Synset('gratifying.s.01'), Synset('odoriferous.s.03'), Synset('sweet.a.07'), Synset('fresh.a.06'), Synset('fresh.s.09'), Synset('sugared.s.01'), Synset('sweetly.r.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "sentence = \"The berries were plump and sweet\"\n",
    "sentence = word_tokenize(sentence)\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "for word in sentence:\n",
    "    # print(word)\n",
    "    # tmp = wordnet.synsets(word)[0].pos()\n",
    "    print(wordnet.synsets(word))\n",
    "    # print(word, \":\", tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'wornet' from 'nltk.corpus' (C:\\Users\\kadyrov\\anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4414d5993f25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Plump\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwornet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'wornet' from 'nltk.corpus' (C:\\Users\\kadyrov\\anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py)"
     ]
    }
   ],
   "source": [
    "## Plump \n",
    "\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "word = choice(words)\n",
    "\n",
    "word\n"
   ]
  },
  {
   "source": [
    "# Part 2 \n",
    "\n",
    "See J&M 3rd Exercise 11.1 (12.1 in 2nd). Parsing with the CKY algorithm in Python is frequently used. Several Python versions are available: Simple Python statistical (CKY) parser (Links to an external site.) not explicitly mentioned in BKL, Stensaethf/CKY-Parser at https://github.com/stensaethf/CKY-Parser (Links to an external site.), with a built-in almost CNF converter, and a more capable one with epsilon productions handling at https://github.com/RobMcH/CYK-Parser (Links to an external site.). Apply them to “Book the cooks who cook the books.” and the longest sentences from Assignment 2. Compare the results to the results from previous assignments. If they do not already show important intermediate results edit them so they do."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exercise 11.1 \n",
    "\n",
    "Draw tree structures for the following ATIS phrases:\n",
    "\n",
    "a. Dallas\n",
    "\n",
    "b. from Denver\n",
    "\n",
    "c. after five p.m.\n",
    "\n",
    "d. arriving in Washington\n",
    "\n",
    "e. early flights\n",
    "\n",
    "f. all redeye flights\n",
    "\n",
    "g. on Thursday\n",
    "\n",
    "h. a one-way fare\n",
    "\n",
    "i. any delays in Denver\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 3\n",
    "\n",
    "See BKL Ch8 Section 86. There is a probabilistic CKY parser (Links to an external site.). Apply it to “Book the cooks who cook the books.” and the longest sentences from Assignment 2. Describe similarity and difference from previous results.  A student has reported that there is something wrong with the code so if you cannot make it work within a reasonable time try to find and use some other statistical parse(s) you can find and if more than one  compare results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stat_parser import Parser\n",
    "\n",
    "parser = Parser()\n",
    "\n",
    "parser.parse(\"Book the cooks who cook the books.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}