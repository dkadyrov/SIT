"""
Author: Marco Mera
DePaul University
"""
import string
import sys
from collections import deque
from nltk import sent_tokenize

class GrammarTree(object):
    '''
    Tree data structure used to represent the grammar tree output generated by the cky algorithm
    '''
    def __init__(self, data):
        self.data = data
        self.left = None
        self.right = None

    def insertLeft(self, new_node):
            self.left = new_node

    def insertRight(self, new_node):
            self.right = new_node

def print_level_order(head, queue = deque()):
    '''
    Helper method used to print tree to console.
    :return: n/a
    '''
    if isinstance(head,str):
        print head
        return
    print head.data
    [queue.append(node) for node in [head.left, head.right] if node]
    if queue:
        print_level_order(queue.popleft(), queue)


def get_binary_rules(grammar):
    '''
    get_binary_rules() searches through the grammar binary rules and returns them in a list
    :param grammar: dictionary of grammar rules
    :return: list of binary grammar rules
    '''
    bin_set = set()
    for rules in grammar:
        if len(rules[1].split(' ')) == 2:
            b, c = rules[1].split(' ')
            bin_set.add((rules[0],b,c))
            bin_set.add((rules[0], c, b))
    return list(bin_set)

def get_non_terms(grammar):
    '''
    get_non_terms() returns a set of the non-terminal rules
    :param grammar: dictionary of grammar rules
    :return: a set of non_terms from the grammar
    '''
    non_terms = set()

    for rules in set(grammar):
        non_terms.add(rules[0])

    return non_terms

def get_words(sentence):
    """
    get_words() takes in a string sentence and splits it into a list of words with the punctuation removed
    :param sentence: sentence (string)
    :return: a list of words with the punctuation removed
    """
    return sentence.translate(None,string.punctuation).split(' ')

def get_lexicon(filename):
    '''
    get_lexicon opens a text file and returns a list of sentences in the file
    :param filename: the file path
    :return: a list of sentence strings
    '''
    f = open(filename,'r')
    text = f.read()
    sentences = sent_tokenize(text)
    return sentences

def init_parse_triangle(number_of_words,number_of_nonterm, fill_value=0):
    '''

    :param number_of_words:
    :param number_of_nonterm:
    :param fill_value: the value used to fill the parse triangle
    :return: a parse triangle with the params used as dimensions
    '''
    return [[[fill_value for i in xrange(number_of_nonterm)] for j in xrange(number_of_words)] for k in xrange(number_of_words)]

def get_grammar(filename):
    '''

    get_grammar() reads in a text file of CNF grammar rules.

    :param filename: text file in CNF format i.e.
                            S -> VP
                            S -> NP VP
    :return: python list with grammar rules as tuples

    '''
    with open(filename,'r') as f:
        lines = f.readlines()
        grammar = [tuple(line.rstrip('\n').split(' -> ')) for line in lines]
    return grammar




def calculate_rule_probabilities(grammar):
    '''

    get_rule_probabilities() calculates the probability of each rule in the grammar.

    :param grammar: a list of tuple representing the grammar rules used to calculate probabilities
    :return: a dictionary of grammar rules and their associated probabilities

    '''

    count = {}
    rule_count = {}
    rule_probs = {}

    for rule in grammar:
        if rule[0] not in count:
            count[rule[0]] = 1
        else:
            count[rule[0]] += 1

    for rule in grammar:
        if rule not in rule_count:
            rule_count[rule] = 1
        else:
            rule_count[rule] += 1

    for entry in rule_count:
        rule_probs[entry] = float(rule_count[entry])/count[entry[0]]

    return rule_probs

def build_tree(start,end,idx,back,non_terms):
    '''
    build_tree() builds tree from the backpointer matrix obtained in the cky() function
    :param start: start index for tree
    :param end: end index for tree
    :param idx: index used to find non_terminal
    :param back: the backpointer matrix
    :param non_terms: a list of non-terminals
    :return:
    '''
    tree = GrammarTree(non_terms[idx])
    node = back[start][end][idx]
    if isinstance(node,tuple):
        split,left_rule,right_rule = node
        tree.insertLeft(build_tree(start,split,left_rule,back,non_terms))
        tree.insertRight(build_tree(split,end,right_rule,back,non_terms))
        return tree
    else:
        if node>0:
            tree.insertLeft(GrammarTree(non_terms[node]))
        return tree


def get_parse_tree(score, back,non_terms):
    '''
    get_parse_tree() calls the build_tree() method
    :param score: score matrix
    :param back: backpointer matrix
    :param non_terms: list of non_terminals
    :return: GrammarTree the final parse tree
    '''
    root_index = score[0][len(score)-1].index(max(score[0][len(score)-1]))
    tree = build_tree(0,len(score)-1,root_index,back,non_terms)
    return tree


def cky(words,grammar,rule_probabilities):
    '''
    cky() takes a sentence and parses it according to the provided grammar.
    :param words: words in the sentence (list)
    :param grammar: list of grammar rules
    :param rule_probabilities: the probabilities of a given grammar rule (dictionary)
    :return: GrammarTree: parse tree with highest probability
    '''

    non_terms = list(get_non_terms(grammar))
    score = init_parse_triangle(len(words)+1,len(non_terms))
    back = init_parse_triangle(len(words)+1,len(non_terms),-1)
    rule_index = {}

    for i,word in enumerate(words):
        rules_used = []
        rules_not_used = []

        for j,A in enumerate(non_terms):
            r = A, '\'' + word + '\''
            if r in grammar:
                score[i][i+1][j] = rule_probabilities[r]
                rules_used.append(j)
                rule_index[A] = j
            else:
                rules_not_used.append(j)
                rule_index[A] = j

        ### Handle Unaries
        rules_used_temp = rules_used[:]
        rules_not_used_temp = rules_not_used[:]
        added = True
        while added:
            added = False
            for a in rules_not_used:
                for b in rules_used:
                    r = non_terms[a], non_terms[b]
                    if r in grammar:
                        prob = rule_probabilities[r] * score[i][i+1][b]
                        if prob > score[i][i+1][a]:
                            score[i][i+1][a] = prob
                            back[i][i+1][a] = b
                            rules_used_temp.append(a)
                            try:
                                rules_not_used_temp.remove(a)
                                rules_used_temp.remove(b)
                            except ValueError:
                                pass
                            added = True

            rules_used = rules_used_temp[:]
            rules_not_used =rules_not_used_temp[:]

    binary_rules = get_binary_rules(grammar)
    for span in xrange(2,len(words)+1):
        for begin in xrange(len(words)+1-span):
            rules_used = []
            end = begin + span
            for split in xrange(begin+1, end):
                for rule in binary_rules:
                    a, b, c = rule_index[rule[0]], rule_index[rule[1]], rule_index[rule[2]]
                    concat_rule = rule[0], ' '.join((rule[1], rule[2]))
                    if concat_rule in grammar:
                        prob = score[begin][split][b] * score[split][end][c] * rule_probabilities[concat_rule]
                    else:
                        continue
                    if prob > score[begin][end][a]:
                        score[begin][end][a] = prob
                        back[begin][end][a] = split, b, c
                        rules_used.append(a)

            ### Handle Unaries
            added = True
            while added:
                added = False
                for a in xrange(len(non_terms)):
                    for b in rules_used:
                        r = non_terms[a], non_terms[b]
                        if r in grammar:
                            prob = rule_probabilities[r] * score[begin][end][b]
                            if prob > score[begin][end][a]:
                                score[begin][end][a] = prob
                                back[begin][end][a] = b
                                added = True

    return get_parse_tree(score,back,non_terms)

if __name__ == '__main__':
    grammar_file = sys.argv[1]
    text_file = sys.argv[2]

    grammar = get_grammar(grammar_file)
    lexicon = get_lexicon(text_file)
    rule_probabilities = calculate_rule_probabilities(grammar)

    for sentence in lexicon:
        word_list = get_words(sentence)
        parse_tree = cky(word_list,grammar,rule_probabilities)
        print_level_order(parse_tree)



